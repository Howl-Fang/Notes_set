ÂéüÂõ†ÔºöÊ≤°Êúâ‰ΩøÁî®Â§öÂç°

<details>
<summary>Question</summary>

(EngineCore_DP0 pid=2273233) INFO 02-28 17:42:14 [gpu_model_runner.py:5140] Encoder cache will be initialized with a budget of 18605 tokens, and profiled with 1 video items of the maximum feature size.
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 113, in __init__
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 128, in determine_available_memory
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 459, in run_method
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return func(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return func(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 339, in determine_available_memory
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     self.model_runner.profile_run()
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5156, in profile_run
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     dummy_encoder_outputs = self.model.embed_multimodal(
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1579, in embed_multimodal
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     video_embeddings = self._process_video_input(multimodal_input)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1526, in _process_video_input
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return run_dp_sharded_mrope_vision_model(
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/vision.py", line 494, in run_dp_sharded_mrope_vision_model
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     image_embeds_local = vision_model(pixel_values_local, local_grid_thw_list)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 767, in forward
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     x = blk(
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]         ^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 405, in forward
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     x = residual + self.mlp(x_fused_norm)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                    ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 224, in forward
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     x = self.act_fn(x)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]         ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 126, in forward
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return self._forward_method(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/activation.py", line 137, in forward_native
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     @staticmethod
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return compiled_fn(full_args)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]                             ^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     outs = compiled_fn(args)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     return self.current_callable(inputs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     out = model(new_inputs)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]   File "/tmp/torchinductor_hligi/ha/chavmznvmjpfllttktjuaogwparltca35vsbxshitafspr6q662w.py", line 100, in call
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]     buf0 = empty_strided_cuda((s77, 1, s53 // 2), (max(1, s53 // 2), max(1, s53 // 2), 1), torch.bfloat16)
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) ERROR 02-28 17:42:17 [core.py:1006] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 698.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 559.00 MiB is free. Including non-PyTorch memory, this process has 23.01 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 614.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables  )
(EngineCore_DP0 pid=2273233) Process EngineCore_DP0:
(EngineCore_DP0 pid=2273233) Traceback (most recent call last):
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/tmp/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=2273233)     self.run()
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/tmp/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=2273233)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=2273233)     raise e
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=2273233)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=2273233)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=2273233)     super().__init__(
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 113, in __init__
(EngineCore_DP0 pid=2273233)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=2273233)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=2273233)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=2273233)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 128, in determine_available_memory
(EngineCore_DP0 pid=2273233)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=2273233)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=2273233)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 459, in run_method
(EngineCore_DP0 pid=2273233)     return func(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=2273233)     return func(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 339, in determine_available_memory
(EngineCore_DP0 pid=2273233)     self.model_runner.profile_run()
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5156, in profile_run
(EngineCore_DP0 pid=2273233)     dummy_encoder_outputs = self.model.embed_multimodal(
(EngineCore_DP0 pid=2273233)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1579, in embed_multimodal
(EngineCore_DP0 pid=2273233)     video_embeddings = self._process_video_input(multimodal_input)
(EngineCore_DP0 pid=2273233)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1526, in _process_video_input
(EngineCore_DP0 pid=2273233)     return run_dp_sharded_mrope_vision_model(
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/vision.py", line 494, in run_dp_sharded_mrope_vision_model
(EngineCore_DP0 pid=2273233)     image_embeds_local = vision_model(pixel_values_local, local_grid_thw_list)
(EngineCore_DP0 pid=2273233)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=2273233)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=2273233)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 767, in forward
(EngineCore_DP0 pid=2273233)     x = blk(
(EngineCore_DP0 pid=2273233)         ^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=2273233)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=2273233)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 405, in forward
(EngineCore_DP0 pid=2273233)     x = residual + self.mlp(x_fused_norm)
(EngineCore_DP0 pid=2273233)                    ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=2273233)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=2273233)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 224, in forward
(EngineCore_DP0 pid=2273233)     x = self.act_fn(x)
(EngineCore_DP0 pid=2273233)         ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=2273233)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=2273233)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 126, in forward
(EngineCore_DP0 pid=2273233)     return self._forward_method(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=2273233)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/activation.py", line 137, in forward_native
(EngineCore_DP0 pid=2273233)     @staticmethod
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=2273233)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=2273233)     return compiled_fn(full_args)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=2273233)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=2273233)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=2273233)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=2273233)                             ^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=2273233)     outs = compiled_fn(args)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=2273233)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=2273233)     return self.current_callable(inputs)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
(EngineCore_DP0 pid=2273233)     out = model(new_inputs)
(EngineCore_DP0 pid=2273233)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233)   File "/tmp/torchinductor_hligi/ha/chavmznvmjpfllttktjuaogwparltca35vsbxshitafspr6q662w.py", line 100, in call
(EngineCore_DP0 pid=2273233)     buf0 = empty_strided_cuda((s77, 1, s53 // 2), (max(1, s53 // 2), max(1, s53 // 2), 1), torch.bfloat16)
(EngineCore_DP0 pid=2273233)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=2273233) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 698.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 559.00 MiB is free. Including non-PyTorch memory, this process has 23.01 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 614.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables  )
[rank0]:[W228 17:42:18.288777459 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown   (function operator())
(APIServer pid=2272936) Traceback (most recent call last):
(APIServer pid=2272936)   File "frozen runpy", line 198, in _run_module_as_main
(APIServer pid=2272936)   File "frozen runpy", line 88, in _run_code
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 531, in module
(APIServer pid=2272936)     uvloop.run(run_server(args))
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=2272936)     return __asyncio.run(
(APIServer pid=2272936)            ^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/tmp/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 195, in run
(APIServer pid=2272936)     return runner.run(main)
(APIServer pid=2272936)            ^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/tmp/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=2272936)     return self._loop.run_until_complete(task)
(APIServer pid=2272936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=2272936)     return await main
(APIServer pid=2272936)            ^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 457, in run_server
(APIServer pid=2272936)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 476, in run_server_worker
(APIServer pid=2272936)     async with build_async_engine_client(
(APIServer pid=2272936)                ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/tmp/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=2272936)     return await anext(self.gen)
(APIServer pid=2272936)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 96, in build_async_engine_client
(APIServer pid=2272936)     async with build_async_engine_client_from_engine_args(
(APIServer pid=2272936)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/tmp/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=2272936)     return await anext(self.gen)
(APIServer pid=2272936)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 137, in build_async_engine_client_from_engine_args
(APIServer pid=2272936)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=2272936)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
(APIServer pid=2272936)     return cls(
(APIServer pid=2272936)            ^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 148, in __init__
(APIServer pid=2272936)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=2272936)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
(APIServer pid=2272936)     return AsyncMPClient(*client_args)
(APIServer pid=2272936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 835, in __init__
(APIServer pid=2272936)     super().__init__(
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 490, in __init__
(APIServer pid=2272936)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=2272936)          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2272936)   File "/localdata/hf/vllm/tmp/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=2272936)     next(self.gen)
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 925, in launch_core_engines
(APIServer pid=2272936)     wait_for_engine_startup(
(APIServer pid=2272936)   File "/localdata/hf/vllm/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
(APIServer pid=2272936)     raise RuntimeError(
(APIServer pid=2272936) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
ËøôÊòØÊñ∞‰∫ßÁîüÁöÑÈîôËØØ
</details>

Âú® vLLM ‰∏≠Âà©Áî®Â§öÂº†ÊòæÂç°‰∏ªË¶ÅÊúâ‰∏§ÁßçÊñπÂºèÔºö**Tensor Parallelism (TP)** Âíå **Data Parallelism (DP)**„ÄÇÈíàÂØπ‰Ω†ÁöÑ `AutoGLM-Phone-9B` Â§öÊ®°ÊÄÅÊ®°ÂûãÔºå‰ª•‰∏ãÊòØÂÆåÊï¥ÈÖçÁΩÆÊåáÂçóÔºö

---

## üîë Ê†∏ÂøÉÊ¶ÇÂøµ

| Âπ∂Ë°åÊñπÂºè | ‰ΩúÁî® | ÈÄÇÁî®Âú∫ÊôØ | ÊòæÂ≠òÊïàÊûú |
|---------|------|---------|---------|
| **TP (Tensor Parallel)** | Â∞ÜÊ®°ÂûãÂ±ÇÂàáÂàÜÂà∞Â§öÂç° | ÂçïÊ®°ÂûãÂ§™Â§ßÔºåÂçïÂç°Êîæ‰∏ç‰∏ã | ‚úÖ ÊòæÂ≠òÁ∫øÊÄßÂàÜÊëä |
| **DP (Data Parallel)** | Â§öÂç°ËøêË°åÁõ∏ÂêåÊ®°ÂûãÔºåÂ§ÑÁêÜ‰∏çÂêåËØ∑Ê±Ç | È´òÂπ∂ÂèëÊé®ÁêÜ | ‚ùå ÊØèÂç°‰ªçÈúÄÂÆåÊï¥Ê®°Âûã |

> üí° **9B Â§öÊ®°ÊÄÅÊ®°ÂûãÊé®Ëçê**Ôºö‰ºòÂÖà‰ΩøÁî® **TP=2**Ôºå2 Âº†Âç°ÂêÑÊâøÊãÖ ~12GB Ê®°ÂûãÊùÉÈáç + ÁºñÁ†ÅÂô®ÔºåËΩªÊùæË∑ëÊª° 24G ÊòæÂ≠ò„ÄÇ

---

## üöÄ Â§öÂç°ÂêØÂä®ÈÖçÁΩÆ

### ÊñπÊ°à AÔºöTensor ParallelismÔºàÊé®Ëçê ‚≠êÔºâ

```bash
# ÂÅáËÆæ‰Ω†Êúâ 2 Âº† GPU (0,1)
export CUDA_VISIBLE_DEVICES=0,1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python3 -m vllm.entrypoints.openai.api_server \
  --served-model-name autoglm-phone-9b \
  --model zai-org/AutoGLM-Phone-9B \
  --port 8000 \
  \
  --tensor-parallel-size 2 \                    # üîë TP=2ÔºåÊ®°ÂûãÂàáÂàÜÂà∞2Âç° \
  --mm-encoder-tp-mode data \                   # ËßÜÈ¢ëÁºñÁ†ÅÂô®‰ΩøÁî® DP Ê®°ÂºèÔºàÈÅøÂÖç TP ÂàáÂàÜÁºñÁ†ÅÂô®Ôºâ \
  --mm_processor_cache_type shm \
  --mm_processor_kwargs "{\"max_pixels\": 2000000}" \
  --max-model-len 20480 \
  --chat-template-content-format string \
  --limit-mm-per-prompt "{\"image\": 4}" \
  \
  --gpu-memory-utilization 0.90 \               # TP Ê®°Âºè‰∏ãÂèØÈÄÇÂΩìÊèêÈ´ò \
  --enforce-eager \                             # Â§öÂç°Êó∂Âª∫ËÆÆÂêØÁî®ÔºåÂáèÂ∞ëÂêåÊ≠•ÂºÄÈîÄ \
  --distributed-executor-backend ray            # Êàñ‰ΩøÁî® 'mp' (multiprocessing)
```

### ÊñπÊ°à BÔºöData ParallelismÔºàÈ´òÂπ∂ÂèëÂú∫ÊôØÔºâ

```bash
# DP ÈúÄË¶Å vLLM v0.6.0+ ‰∏îÈÖçÁΩÆÊõ¥Â§çÊùÇÔºåÈÄöÂ∏∏ÈÖçÂêà TP ‰ΩøÁî®
# Á§∫‰æãÔºö2 ËäÇÁÇπ √ó 2 Âç° = 4 ÂÆû‰æãÔºåÊØèÂÆû‰æã TP=1
export CUDA_VISIBLE_DEVICES=0  # ÂÆû‰æã1
python3 -m vllm.entrypoints.openai.api_server --port 8000 --tensor-parallel-size 1 ... &

export CUDA_VISIBLE_DEVICES=1  # ÂÆû‰æã2  
python3 -m vllm.entrypoints.openai.api_server --port 8001 --tensor-parallel-size 1 ... &
# ÂâçÁ´ØÁî® Nginx/Ë¥üËΩΩÂùáË°°Âô®ÂàÜÂèëËØ∑Ê±ÇÂà∞ 8000/8001
```

---

## ‚öôÔ∏è ÂÖ≥ÈîÆÂèÇÊï∞ËØ¶Ëß£

```bash
# 1. Âü∫Á°ÄÂπ∂Ë°å
--tensor-parallel-size 2          # ‰ΩøÁî® 2 Âº†Âç°ÂÅöÊ®°ÂûãÂπ∂Ë°åÔºàÊúÄÂ∏∏Áî®Ôºâ
--pipeline-parallel-size 1        # PP ÈÄöÂ∏∏ËÆæ‰∏∫ 1ÔºåÈô§ÈùûÊ®°ÂûãÊûÅÂ§ß (>70B)

# 2. Â§öÊ®°ÊÄÅÁâπÊÆäÈÖçÁΩÆÔºàAutoGLM ÈáçÁÇπÔºÅÔºâ
--mm-encoder-tp-mode data         # ‚úÖ ÁºñÁ†ÅÂô®‰∏çÂàáÂàÜÔºåÊØèÂç°Áã¨Á´ãÂ§ÑÁêÜÔºàÂÖºÂÆπÊÄßÂ•ΩÔºâ
# --mm-encoder-tp-mode tensor     # ‚ùå Â∞ùËØïÂàáÂàÜÁºñÁ†ÅÂô®ÔºàÂèØËÉΩÊä•ÈîôÔºåÈúÄÊ®°ÂûãÊîØÊåÅÔºâ

--mm_processor_cache_type shm     # Â§öÂç°ÂÖ±‰∫´ÂÜÖÂ≠òÁºìÂ≠òÔºåÂáèÂ∞ëÈáçÂ§çÂä†ËΩΩ

# 3. ÂàÜÂ∏ÉÂºèÂêéÁ´Ø
--distributed-executor-backend ray   # Êé®ËçêÔºöRay ÁÆ°ÁêÜÂ§öËøõÁ®ãÊõ¥Á®≥ÂÆö
# --distributed-executor-backend mp  # Â§áÈÄâÔºömultiprocessingÔºàÁÆÄÂçïÂú∫ÊôØÔºâ

# 4. ÊòæÂ≠ò‰ºòÂåñÔºàÂ§öÂç°‰ªçÈúÄÊ≥®ÊÑèÔºâ
--gpu-memory-utilization 0.90   # TP Ê®°Âºè‰∏ãÂèØËÆæÊõ¥È´òÔºà0.85~0.92Ôºâ
--kv-cache-dtype fp8            # Â¶ÇÁ°¨‰ª∂ÊîØÊåÅÔºåKV cache Áî® fp8 ÁúÅ 50% ÊòæÂ≠ò
```

---

## üîç È™åËØÅÂ§öÂç°ÊòØÂê¶ÁîüÊïà

ÂêØÂä®ÂêéËßÇÂØüÊó•ÂøóÔºö
```log
INFO ... World size: 2, Tensor parallel size: 2    # ‚úÖ TP=2 ÁîüÊïà
INFO ... # GPU blocks: 1234 per GPU                # ‚úÖ ÊØèÂç°Áã¨Á´ã block Êï∞
INFO ... Distributed execution with backend: ray   # ‚úÖ ÂêéÁ´ØÊ≠£Â∏∏
```

ÊàñÁî®ÂëΩ‰ª§Ê£ÄÊü•Ôºö
```bash
# Êü•ÁúãËøõÁ®ãÂç†Áî® GPU
nvidia-smi pmon -c 1

# Êü•Áúã NCCL ÈÄö‰ø°ÔºàTP ‰æùËµñÔºâ
nccl-debug=info python3 -m vllm.entrypoints.openai.api_server ... 2>&1 | grep NCCL
```

---

## üõ†Ô∏è Â∏∏ËßÅÈóÆÈ¢òÊéíÊü•

### ‚ùå ÈóÆÈ¢ò 1Ôºö`NCCL connection error` / `connection refused`
```bash
# Ëß£ÂÜ≥ÊñπÊ°àÔºö
export NCCL_SOCKET_IFNAME=eth0          # ÊåáÂÆöÁΩëÂç°ÔºàÂ§öÊú∫Êó∂ÂøÖÈúÄÔºâ
export NCCL_IB_DISABLE=1                # Êó† InfiniBand Êó∂Á¶ÅÁî®
export NCCL_DEBUG=INFO                  # Êü•ÁúãËØ¶ÁªÜÈÄö‰ø°Êó•Âøó
```

### ‚ùå ÈóÆÈ¢ò 2ÔºöÂ§öÂç°‰ΩÜÊòæÂ≠òÊ≤°ÂàÜÊëäÔºà‰ªç OOMÔºâ
```bash
# Ê£ÄÊü•ÁÇπÔºö
# 1. Á°ÆËÆ§ --tensor-parallel-size Â∑≤ËÆæÁΩÆ
# 2. Á°ÆËÆ§ CUDA_VISIBLE_DEVICES ÂåÖÂê´Â§öÂç°
# 3. Â§öÊ®°ÊÄÅÁºñÁ†ÅÂô®ÂèØËÉΩÊú™ÂàáÂàÜÔºöÂ∞ùËØï --mm-encoder-tp-mode tensorÔºàÂ¶ÇÊ®°ÂûãÊîØÊåÅÔºâ
# 4. Èôç‰Ωé max_pixels / max-model-len ÂÖàÈ™åËØÅ TP Âü∫Á°ÄÂäüËÉΩ
```

### ‚ùå ÈóÆÈ¢ò 3ÔºöRay ÂêØÂä®Â§±Ë¥•
```bash
# ÊâãÂä®ÂàùÂßãÂåñ RayÔºàË∞ÉËØïÁî®ÔºâÔºö
ray start --head --port=6379 --num-gpus=2
# ÁÑ∂ÂêéÂú® vLLM ÂëΩ‰ª§‰∏≠Âä†Ôºö
--ray-address=auto
```

---

## üìä ‰∏çÂêå GPU Êï∞ÈáèÊé®ËçêÈÖçÁΩÆ

| GPU Êï∞Èáè | Êé®Ëçê TP | ÊòæÂ≠òÊî∂Áõä | Â§áÊ≥® |
|---------|---------|---------|------|
| 1√ó24G | TP=1 | Âü∫ÂáÜ | ÈúÄ‰∏•Ê†ºÈôêÂà∂ `max_pixels‚â§1M` |
| **2√ó24G** | **TP=2** | ‚úÖ Ê®°ÂûãÊùÉÈáçÂáèÂçä | ‚≠ê ÊúÄ‰Ω≥ÊÄß‰ª∑ÊØîÔºåÊé®Ëçê |
| 4√ó24G | TP=2 + DP=2 | ‚úÖ ÊòæÂ≠ò+ÂêûÂêêÂèåÊèêÂçá | ÈúÄÈÖçÁΩÆË¥üËΩΩÂùáË°° |
| 2√ó48G (A100) | TP=2 | ‚úÖ ÂèØÂºÄÊõ¥Â§ß context | ÈÄÇÂêàÈïøËßÜÈ¢ëÁêÜËß£ |

---

## üéØ ÊúÄÁªàÊé®ËçêÂëΩ‰ª§Ôºà2 Âç° 24GÔºâ

```bash
#!/bin/bash
export CUDA_VISIBLE_DEVICES=0,1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export NCCL_DEBUG=WARNING  # Áîü‰∫ßÁéØÂ¢ÉËÆæ‰∏∫ WARNING ÂáèÂ∞ëÊó•Âøó

python3 -m vllm.entrypoints.openai.api_server \
  --model zai-org/AutoGLM-Phone-9B \
  --served-model-name autoglm-phone-9b \
  --port 8000 \
  --tensor-parallel-size 2 \
  --mm-encoder-tp-mode data \
  --mm_processor_cache_type shm \
  --mm_processor_kwargs "{\"max_pixels\": 2000000, \"max_frames\": 16}" \
  --max-model-len 20480 \
  --chat-template-content-format string \
  --limit-mm-per-prompt "{\"image\": 4, \"video\": 1}" \
  --gpu-memory-utilization 0.90 \
  --enforce-eager \
  --distributed-executor-backend ray \
  --host 0.0.0.0
```

> ‚úÖ Ê≠§ÈÖçÁΩÆ‰∏ãÔºö  
> - Ê®°ÂûãÊùÉÈáç ~9GB ‚Üí ÊØèÂç° ~4.5GB  
> - ËßÜÈ¢ëÁºñÁ†ÅÂô® ~3GB/Âç°  
> - KV cache + ÊøÄÊ¥ªÂÄº ~10GB/Âç°  
> - **ÊÄªËÆ° ~17.5GB/Âç° < 24GB**ÔºåÁ®≥ÂÆöËøêË°å ‚úÖ

Â¶ÇÊûú‰ªçÊúâÈóÆÈ¢òÔºåËØ∑Êèê‰æõÔºö
1. `nvidia-smi` ËæìÂá∫ÔºàGPU ÂûãÂè∑/Êï∞ÈáèÔºâ
2. `vllm` ÁâàÊú¨Ôºö`pip show vllm | grep Version`
3. ÂÆåÊï¥ÂêØÂä®Êó•ÂøóÂâç 50 Ë°å

ÊàëÂèØ‰ª•Â∏Æ‰Ω†Ëøõ‰∏ÄÊ≠•Ë∞É‰ºòÔºÅüîß

>Áé∞Âú®ÊàëÁî®ÁöÑÂëΩ‰ª§ÊòØ:
>python3 -m vllm.entrypoints.openai.api_server \
  --served-model-name autoglm-phone-9b \
  --allowed-local-media-path / \
  --mm-encoder-tp-mode data \
  --mm_processor_cache_type shm \
  --mm_processor_kwargs "{\"max_pixels\":5000000}" \
  --max-model-len 25480 \
  --chat-template-content-format string \
  --limit-mm-per-prompt "{\"image\":10}" \
  --model zai-org/AutoGLM-Phone-9B \
  --port 8888

ÊàëÊÄªÂÖ±Êúâ8Âº†3090ÔºåÂ∏ÆÊàë‰øÆÊîπÂëΩ‰ª§

ÈíàÂØπ‰Ω† **8 Âº† RTX 3090 (24GB)** ÁöÑÈÖçÁΩÆÔºåËøêË°å `zai-org/AutoGLM-Phone-9B` Â§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊàë‰∏∫‰Ω†Êèê‰æõ‰∏§Â•óÊñπÊ°àÔºö

---

## üéØ ÊñπÊ°à‰∏ÄÔºöÂçïÂÆû‰æã TP=4ÔºàÊé®Ëçê ‚≠ê Á®≥ÂÆö‰ºòÂÖàÔºâ

‰ΩøÁî® 4 Âº†Âç°ÂÅö **Tensor Parallelism**ÔºåÊ®°ÂûãÊùÉÈáçÂàÜÊëäÔºåÊòæÂ≠òÂéãÂäõÊúÄÂ∞èÔºåÈÄÇÂêàÁîü‰∫ßÁéØÂ¢É„ÄÇ

```bash
#!/bin/bash
# ‰ΩøÁî®Ââç 4 Âº†Âç° (0,1,2,3)
export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export NCCL_DEBUG=WARNING
export RAY_DEDUP_LOGS=1

python3 -m vllm.entrypoints.openai.api_server \
  --model zai-org/AutoGLM-Phone-9B \
  --served-model-name autoglm-phone-9b \
  --port 8888 \
  --host 0.0.0.0 \
  \
  --tensor-parallel-size 4 \                           # üîë 4 Âç°Ê®°ÂûãÂπ∂Ë°å \
  --mm-encoder-tp-mode data \                          # ÁºñÁ†ÅÂô®ÊØèÂç°Áã¨Á´ãÂä†ËΩΩÔºàÂÖºÂÆπÊÄßÂ•ΩÔºâ \
  --mm_processor_cache_type shm \                      # Â§öÂç°ÂÖ±‰∫´ÂÜÖÂ≠òÁºìÂ≠ò \
  --mm_processor_kwargs "{\"max_pixels\": 2000000, \"max_frames\": 16}" \  # 500 ‰∏á‚Üí200 ‰∏áÂÉèÁ¥† \
  --max-model-len 20480 \                              # 25480‚Üí20480Ôºå‰øùÂÆàÊòæÂ≠ò \
  --chat-template-content-format string \
  --limit-mm-per-prompt "{\"image\": 4, \"video\": 1}" \  # 10 Âº†‚Üí4 Âº†ÂõæÁâáÂπ∂Âèë \
  \
  --gpu-memory-utilization 0.88 \                      # È¢ÑÁïô 12% ÁªôÁºñÁ†ÅÂô®Â≥∞ÂÄº \
  --enforce-eager \                                    # Â§öÂç°Êó∂ÂáèÂ∞ëÂÜÖÂ≠òÁ¢éÁâá \
  --distributed-executor-backend ray \                 # Ray ÁÆ°ÁêÜÂ§öËøõÁ®ã \
  --enable-prefix-caching \                            # ÂºÄÂêØ prompt ÁºìÂ≠òÔºàÊèêÂçáÂ§öËΩÆÂØπËØùÊïàÁéáÔºâ\
  --swap-space 4                                       # 4GB CPU ‰∫§Êç¢Á©∫Èó¥ÔºàÈò≤ OOM ÁºìÂÜ≤Ôºâ
```

### ‚úÖ ÊòæÂ≠ò‰º∞ÁÆóÔºàTP=4ÔºåÊØèÂç° 24GBÔºâ
| ÁªÑ‰ª∂ | ÊØèÂç°Âç†Áî® | ËØ¥Êòé |
|------|---------|------|
| Ê®°ÂûãÊùÉÈáç (BF16) | ~4.5 GB | 9B / 4 Âç° |
| ËßÜÈ¢ëÁºñÁ†ÅÂô® | ~3.5 GB | data mode ‰∏ãÊØèÂç°Áã¨Á´ãÂä†ËΩΩ |
| KV Cache + ÊøÄÊ¥ª | ~10-12 GB | max-model-len=20480 |
| **ÊÄªËÆ°** | **~18-20 GB** | ‚úÖ 24GB ÊòæÂ≠òÂÜÖÁ®≥ÂÆöËøêË°å |

---

## üöÄ ÊñπÊ°à‰∫åÔºö8 Âç°ÂÖ®Áî®ÔºàÈ´òÂêûÂêê DP+TPÔºâ

ÂêØÂä® **2 ‰∏™Áã¨Á´ãÂÆû‰æã**ÔºåÊØè‰∏™ TP=4ÔºåÂàÜÂà´ÁªëÂÆö‰∏çÂêå GPUÔºåÂâçÁ´ØÁî® Nginx Ë¥üËΩΩÂùáË°°„ÄÇ

### ÂÆû‰æã 1ÔºàGPU 0-3ÔºåÁ´ØÂè£ 8888Ôºâ
```bash
# start_instance_1.sh
export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python3 -m vllm.entrypoints.openai.api_server \
  --model zai-org/AutoGLM-Phone-9B \
  --served-model-name autoglm-phone-9b \
  --port 8888 \
  --tensor-parallel-size 4 \
  --mm-encoder-tp-mode data \
  --mm_processor_cache_type shm \
  --mm_processor_kwargs "{\"max_pixels\": 2000000}" \
  --max-model-len 20480 \
  --limit-mm-per-prompt "{\"image\": 4}" \
  --gpu-memory-utilization 0.88 \
  --enforce-eager \
  --distributed-executor-backend ray \
  --ray-namespace autoglm_instance1 &  # ÈöîÁ¶ª Ray ÂëΩÂêçÁ©∫Èó¥
```

### ÂÆû‰æã 2ÔºàGPU 4-7ÔºåÁ´ØÂè£ 8889Ôºâ
```bash
# start_instance_2.sh
export CUDA_VISIBLE_DEVICES=4,5,6,7
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python3 -m vllm.entrypoints.openai.api_server \
  --model zai-org/AutoGLM-Phone-9B \
  --served-model-name autoglm-phone-9b \
  --port 8889 \
  --tensor-parallel-size 4 \
  --mm-encoder-tp-mode data \
  --mm_processor_cache_type shm \
  --mm_processor_kwargs "{\"max_pixels\": 2000000}" \
  --max-model-len 20480 \
  --limit-mm-per-prompt "{\"image\": 4}" \
  --gpu-memory-utilization 0.88 \
  --enforce-eager \
  --distributed-executor-backend ray \
  --ray-namespace autoglm_instance2 &
```

### üîÅ Nginx Ë¥üËΩΩÂùáË°°ÈÖçÁΩÆÔºàÂèØÈÄâÔºâ
```nginx
# /etc/nginx/conf.d/autoglm.conf
upstream autoglm_backend {
    least_conn;  # ÊúÄÂ∞ëËøûÊé•Êï∞Ë∞ÉÂ∫¶
    server 127.0.0.1:8888;
    server 127.0.0.1:8889;
}

server {
    listen 80;
    location / {
        proxy_pass http://autoglm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        client_max_body_size 100M;  # ÊîØÊåÅÂ§ßÂõæÁâá/ËßÜÈ¢ë‰∏ä‰º†
    }
}
```

---

## ‚öôÔ∏è ÂèÇÊï∞Ë∞É‰ºòÂª∫ËÆÆÔºà3090 ‰∏ìÂ±ûÔºâ

```bash
# üî∏ Â§öÊ®°ÊÄÅÂèÇÊï∞ÔºàÂÖ≥ÈîÆÔºÅ3090 ÊòæÂ≠òÊúâÈôêÔºâ
--mm_processor_kwargs "{\"max_pixels\": 2000000, \"max_frames\": 16, \"min_pixels\": 256000}"
# max_pixels: 200 ‰∏á ‚âà 1920√ó1080ÔºåÂπ≥Ë°°ÁîªË¥®‰∏éÊòæÂ≠ò
# max_frames: ËßÜÈ¢ëÊúÄÂ§öÈááÊ†∑ 16 Â∏ßÔºåÈÅøÂÖçÈïøËßÜÈ¢ë OOM

# üî∏ Âπ∂ÂèëÊéßÂà∂
--limit-mm-per-prompt "{\"image\": 4, \"video\": 1}"
--max-num-seqs 128  # ÂçïÂÆû‰æãÊúÄÂ§ßÂπ∂ÂèëËØ∑Ê±ÇÊï∞Ôºà3090 Âª∫ËÆÆ 64~128Ôºâ

# üî∏ ÊòæÂ≠ò‰ºòÂåñ
--gpu-memory-utilization 0.88      # 3090 Âª∫ËÆÆ 0.85~0.90ÔºåÁïô‰ΩôÈáèÁªôÁºñÁ†ÅÂô®
--kv-cache-dtype fp8              # ‚úÖ Â¶Ç vLLM+PyTorch ÊîØÊåÅÔºåKV cache ÁúÅ 50% ÊòæÂ≠ò
--enforce-eager                   # Â§öÂç°Êó∂Á¶ÅÁî® CUDA GraphÔºåÂáèÂ∞ëÁ¢éÁâá

# üî∏ ÂàÜÂ∏ÉÂºèÈÄö‰ø°‰ºòÂåñÔºà8 Âç°/Â§öÊú∫ÂøÖÂ§áÔºâ
export NCCL_SOCKET_IFNAME=eth0           # ÊåáÂÆöÈÄö‰ø°ÁΩëÂç°
export NCCL_IB_DISABLE=1                 # Êó† InfiniBand Êó∂Á¶ÅÁî®
export NCCL_MIN_NRINGS=4                 # Â¢ûÂä†ÈÄö‰ø°ÁéØÊï∞ÔºåÊèêÂçá TP ÊïàÁéá
```

---

## üîç ÂêØÂä®ÂêéÈ™åËØÅ

```bash
# 1. Ê£ÄÊü• GPU Âç†Áî®
watch -n 1 nvidia-smi

# 2. Êü•Áúã vLLM Êó•ÂøóÂÖ≥ÈîÆË°å
# ‚úÖ Â∫îÁúãÂà∞Ôºö
# "World size: 4, Tensor parallel size: 4"
# "# GPU blocks: XXX per GPU"
# "Loading model ... on 4 GPUs"

# 3. ÊµãËØï API
curl http://localhost:8888/v1/models  # Êü•ÁúãÊ®°ÂûãÊòØÂê¶Â∞±Áª™

# 4. ÂéãÂäõÊµãËØïÔºàÂèØÈÄâÔºâ
# Áî® locust Êàñ ab ÊµãËØï QPSÔºåËßÇÂØüÊòæÂ≠ò/Âª∂ËøüÂèòÂåñ
```

---

## üõ†Ô∏è Â∏∏ËßÅÈóÆÈ¢òÈÄüÊü•

| ÈóÆÈ¢ò | ÂèØËÉΩÂéüÂõ† | Ëß£ÂÜ≥ÊñπÊ°à |
|------|---------|---------|
| `NCCL unhandled system error` | ÁΩëÂç°/PCIe ÈÄö‰ø°ÈóÆÈ¢ò | `export NCCL_SOCKET_IFNAME=eth0` + Ê£ÄÊü• `dmesg \| grep NVRM` |
| Â§öÂç°‰ΩÜÊòæÂ≠òÊ≤°ÂàÜÊëä | `--tensor-parallel-size` Êú™ÁîüÊïà | Á°ÆËÆ§ÂëΩ‰ª§Âê´ `--tensor-parallel-size 4` + `CUDA_VISIBLE_DEVICES` |
| ËßÜÈ¢ëÊé®ÁêÜ OOM | ÁºñÁ†ÅÂô®Â≥∞ÂÄºË∂ÖÈôê | Èôç‰Ωé `max_pixels` / `max_frames` + Âä† `--swap-space 4` |
| Ray ÂêØÂä®Âç°‰Ωè | Á´ØÂè£ÂÜ≤Á™Å/ÊùÉÈôê | `ray stop --force` Ê∏ÖÁêÜ + Áî® `--ray-namespace` ÈöîÁ¶ª |

---

## üìä 8√ó3090 ÈÖçÁΩÆÊé®ËçêÊÄªÁªì

| ÁõÆÊ†á | Êé®ËçêÈÖçÁΩÆ | È¢ÑÊúüÊïàÊûú |
|------|---------|---------|
| ‚úÖ **Á®≥ÂÆöËøêË°å** | ÂçïÂÆû‰æã TP=4ÔºàÊñπÊ°à‰∏ÄÔºâ | ÂçïËØ∑Ê±ÇÂª∂Ëøü‰ΩéÔºåÊòæÂ≠òÂÖÖË£ïÔºåÈÄÇÂêà API ÊúçÂä° |
| üöÄ **È´òÂêûÂêê** | ÂèåÂÆû‰æã TP=4 + NginxÔºàÊñπÊ°à‰∫åÔºâ | QPS √ó2ÔºåÈÄÇÂêàÊâπÈáèÊé®ÁêÜ/È´òÂπ∂ÂèëÂú∫ÊôØ |
| üî¨ **ÂÆûÈ™åË∞ÉËØï** | TP=2 + Â§ß context | Áî® 2 Âç°Ë∑ë `max-model-len=32768`ÔºåÁ†îÁ©∂ÈïøËßÜÈ¢ëÁêÜËß£ |

> üí° **ÁªèÈ™åÂª∫ËÆÆ**ÔºöÂÖàÁî®**ÊñπÊ°à‰∏Ä**È™åËØÅÊ®°ÂûãËÉΩÁ®≥ÂÆöÂä†ËΩΩ + Êé®ÁêÜÔºåÂÜçÊâ©Â±ïÂà∞ÊñπÊ°à‰∫å„ÄÇAutoGLM-Phone ÁöÑËßÜÈ¢ëÁºñÁ†ÅÂô®ÂØπÊòæÂ≠òÊïèÊÑüÔºå**ÂÆÅÂèØ‰øùÂÆàÂèÇÊï∞ÔºåÈÅøÂÖçÂèçÂ§ç OOM ÈáçÂêØ**„ÄÇ

Â¶ÇÊûúÈúÄË¶ÅÊàëÂ∏Æ‰Ω†Ôºö
1. ÂÜô‰∏Ä‰∏™Ëá™Âä®ÁõëÊéßÊòæÂ≠ò + ÈáçÂêØÁöÑÂÆàÊä§ËÑöÊú¨Ôºü
2. ÈÖçÁΩÆ Prometheus + Grafana ÁõëÊéß vLLM ÊåáÊ†áÔºü
3. ÈíàÂØπÁ∫ØÂõæÁâá/Á∫ØËßÜÈ¢ëÂú∫ÊôØËøõ‰∏ÄÊ≠•‰ºòÂåñÂèÇÊï∞Ôºü

ÈöèÊó∂ÂëäËØâÊàëÔºÅüîß